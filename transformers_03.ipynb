{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcVem9yyAN9jZTG3dvRKus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomJenkin/playground/blob/main/transformers_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bXWvG-5XQTwZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "from sklearn import cluster"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "ifvumPP-rtP6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()"
      ],
      "metadata": {
        "id": "MPLmOWqYfT-f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    sp500 = yf.Ticker(\"^GSPC\")\n",
        "    data = sp500.history(period=\"max\")\n",
        "\n",
        "    cob_start = '20200101'\n",
        "    cob_end = None\n",
        "    n_clusters = 20\n",
        "    ds = data.Close.pct_change().rename(0).loc[slice(cob_start,cob_end)]\n",
        "    dr = ds.to_frame()\n",
        "    for n in range(1,5):\n",
        "        dr[-n] = dr[0].shift(n)\n",
        "    dr = dr.sort_index(axis=1).dropna()\n",
        "    model = cluster.KMeans(n_clusters=n_clusters,random_state=0).fit(dr)\n",
        "    dr = dr.assign(label = model.labels_)\n",
        "    dm = dr.label.value_counts().reset_index().rename_axis('label2').reset_index()\n",
        "    dm = dm.assign(label2 = dm.label2+1)\n",
        "    dmr = dm.set_index('label').label2.to_dict()\n",
        "    dr = dr.assign(label2 = dr.label.map(dmr))\n",
        "    labels = dr.label2.to_list()\n",
        "    print(dr.shape)\n",
        "\n",
        "    prompts = [\n",
        "        labels[0:100],\n",
        "        labels[100:200],\n",
        "        labels[200:300],\n",
        "    ]"
      ],
      "metadata": {
        "id": "grlnHh0wXhHO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------\n",
        "# Positional Encodings (sin/cos as in the paper)\n",
        "# -----------------------\n",
        "def sinusoidal_position_encoding(max_len: int, d_model: int, device=None):\n",
        "    pe = torch.zeros(max_len, d_model, device=device)\n",
        "    position = torch.arange(0, max_len, device=device).unsqueeze(1)  # [T, 1]\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device) *\n",
        "                         (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe  # [T, d_model]\n",
        "\n",
        "# -----------------------\n",
        "# Multi-Head Self-Attention (causal)\n",
        "# -----------------------\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x: [B, T, d_model]\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)  # [B, T, 3C]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape -> [B, n_heads, T, d_head]\n",
        "        def split_heads(t):\n",
        "            return t.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        q = split_heads(q)\n",
        "        k = split_heads(k)\n",
        "        v = split_heads(v)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # [B, h, T, T]\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = attn @ v  # [B, h, T, d_head]\n",
        "\n",
        "        # concat heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # [B, T, C]\n",
        "        out = self.proj_drop(self.out(out))\n",
        "        return out\n",
        "\n",
        "# -----------------------\n",
        "# Position-wise FeedForward\n",
        "# -----------------------\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.drop(F.relu(self.fc1(x))))  # ReLU as in the paper\n",
        "\n",
        "# -----------------------\n",
        "# Transformer Block (Pre-LN for stability)\n",
        "# -----------------------\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.sa = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        x = x + self.drop(self.sa(self.ln1(x), attn_mask=attn_mask))\n",
        "        x = x + self.drop(self.ff(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "# -----------------------\n",
        "# Simplified Decoder-Only Transformer LM\n",
        "# -----------------------\n",
        "class SimpleTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, d_ff=1024,\n",
        "                 max_len=512, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(\n",
        "            sinusoidal_position_encoding(max_len, d_model),\n",
        "            freeze=True\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # weight tying\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "\n",
        "    def _make_attention_mask(self, x):\n",
        "        \"\"\"\n",
        "        Build a combined mask:\n",
        "        - causal mask to block attending to future positions\n",
        "        - key padding mask to block attending to pads in K/V\n",
        "        returns: mask with True where attention is NOT allowed, shape [B, 1, T, T]\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # causal: [T, T] True above diagonal (i<j)\n",
        "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
        "        causal = causal.unsqueeze(0).unsqueeze(0)  # [1,1,T,T]\n",
        "\n",
        "        # key padding: mask positions that are pad in keys\n",
        "        key_pad = (x == self.pad_id).unsqueeze(1).unsqueeze(2)  # [B,1,1,T]\n",
        "\n",
        "        # broadcast OR\n",
        "        attn_mask = causal | key_pad  # [B,1,T,T]\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: LongTensor [B, T] with token ids (may include pad_id).\n",
        "        Returns logits for all positions: [B, T, vocab_size]\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(f\"Sequence length {T} exceeds max_len {self.max_len}\")\n",
        "\n",
        "        device = x.device\n",
        "        pos = torch.arange(T, device=device)\n",
        "        h = self.tok_emb(x) * math.sqrt(self.d_model) + self.pos_emb(pos)  # [B,T,C]\n",
        "        h = self.drop(h)\n",
        "\n",
        "        attn_mask = self._make_attention_mask(x)  # [B,1,T,T]\n",
        "        for blk in self.blocks:\n",
        "            h = blk(h, attn_mask=attn_mask)\n",
        "\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)  # [B,T,V]\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_next_token(self, x, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Returns next-token logits/probs for each sequence.\n",
        "        x: LongTensor [B, T]\n",
        "        \"\"\"\n",
        "        logits = self.forward(x)[:, -1, :]  # last position per sequence\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "        if top_k is not None:\n",
        "            # top-k filtering for sampling; keep it generic\n",
        "            topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            logits = mask.scatter(1, topk_idx, topk_vals)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)  # [B, V]\n",
        "        next_token = torch.argmax(probs, dim=-1)  # greedy\n",
        "        return next_token, probs  # ints [B], floats [B,V]\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # batch of sequences (variable lengths, padded with pad_id)\n",
        "    batch = [\n",
        "        [101, 42, 77, 88, 5],\n",
        "        [101, 19, 23],\n",
        "        [101, 202, 13, 9, 17, 4, 55]\n",
        "    ]\n",
        "\n",
        "    batch = [\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*60,\n",
        "        [1,1,2]*40,\n",
        "        [1,1,2]*20,\n",
        "        [1,1,2]*1,\n",
        "    ]\n",
        "\n",
        "    #batch = labels\n",
        "\n",
        "    vocab = sorted(set.union(*[set(e) for e in batch]))\n",
        "\n",
        "    # toy config\n",
        "    assert min(vocab) > 0\n",
        "    vocab_size = max(vocab) + 1\n",
        "    # vocab_size = 32000\n",
        "    pad_id = 0\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = SimpleTransformerLM(\n",
        "        vocab_size=vocab_size,\n",
        "        #d_model=256,\n",
        "        #n_layers=4,\n",
        "        #n_heads=4,\n",
        "        #d_ff=1024,\n",
        "        #max_len=256,\n",
        "        d_model=256,\n",
        "        n_layers=4,\n",
        "        n_heads=4,\n",
        "        d_ff=1024,\n",
        "        max_len=2048,\n",
        "        dropout=0.1,\n",
        "        pad_id=pad_id\n",
        "    ).to(device)\n",
        "\n",
        "    # pad right to same length\n",
        "    maxT = max(len(s) for s in batch)\n",
        "    x = torch.full((len(batch), maxT), pad_id, dtype=torch.long)\n",
        "    for i, s in enumerate(batch):\n",
        "        x[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "    x = x.to(device)\n",
        "\n",
        "    # ---- quick warm-up training so the model learns 1,1,2 -> 1 ----\n",
        "    model.train()\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    # teacher-forcing targets: predict next token\n",
        "    # inp means inputs, targ means target\n",
        "    inp  = x[:, :-1]          # [B, T-1]\n",
        "    targ = x[:, 1:]           # [B, T-1]\n",
        "\n",
        "    steps = 100\n",
        "    for _ in range(steps):\n",
        "        logits = model(inp)               # [B, T-1, V]\n",
        "        loss = loss_fn(logits.reshape(-1, model.vocab_size), targ.reshape(-1))\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    model.eval()\n",
        "    # ---- end warm-up training ----\n",
        "\n",
        "    # predict the next token for each sequence\n",
        "    next_ids, next_probs = model.predict_next_token(x, temperature=1.0)\n",
        "    print(\"Next token ids:\", next_ids.tolist())\n",
        "    # next_probs[i] is the full distribution for sequence i\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJlCyKlcQe5H",
        "outputId": "14982142-e42c-416b-da64-c4467c920678"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token ids: [1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWMk3rsvi5EU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_ids, next_probs = model.predict_next_token(x, temperature=1.0)\n",
        "print(\"Next token ids:\", next_ids.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx-xDWoqnSNP",
        "outputId": "80e375ce-d232-4d41-91a4-c348f795db12"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token ids: [1, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(next_probs.cpu()).style.format('{:,.2%}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "tEosrgLonoMu",
        "outputId": "3fb934a1-cab6-4a40-a55e-bc66df52804c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78578dcfe8a0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_32ab1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_32ab1_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
              "      <th id=\"T_32ab1_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
              "      <th id=\"T_32ab1_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_32ab1_row0_col0\" class=\"data row0 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row0_col1\" class=\"data row0 col1\" >100.00%</td>\n",
              "      <td id=\"T_32ab1_row0_col2\" class=\"data row0 col2\" >0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_32ab1_row1_col0\" class=\"data row1 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row1_col1\" class=\"data row1 col1\" >100.00%</td>\n",
              "      <td id=\"T_32ab1_row1_col2\" class=\"data row1 col2\" >0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_32ab1_row2_col0\" class=\"data row2 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row2_col1\" class=\"data row2 col1\" >100.00%</td>\n",
              "      <td id=\"T_32ab1_row2_col2\" class=\"data row2 col2\" >0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_32ab1_row3_col0\" class=\"data row3 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row3_col1\" class=\"data row3 col1\" >100.00%</td>\n",
              "      <td id=\"T_32ab1_row3_col2\" class=\"data row3 col2\" >0.00%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_32ab1_row4_col0\" class=\"data row4 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row4_col1\" class=\"data row4 col1\" >98.08%</td>\n",
              "      <td id=\"T_32ab1_row4_col2\" class=\"data row4 col2\" >1.92%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_32ab1_row5_col0\" class=\"data row5 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row5_col1\" class=\"data row5 col1\" >98.07%</td>\n",
              "      <td id=\"T_32ab1_row5_col2\" class=\"data row5 col2\" >1.93%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_32ab1_row6_col0\" class=\"data row6 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row6_col1\" class=\"data row6 col1\" >98.07%</td>\n",
              "      <td id=\"T_32ab1_row6_col2\" class=\"data row6 col2\" >1.93%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_32ab1_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_32ab1_row7_col0\" class=\"data row7 col0\" >0.00%</td>\n",
              "      <td id=\"T_32ab1_row7_col1\" class=\"data row7 col1\" >97.95%</td>\n",
              "      <td id=\"T_32ab1_row7_col2\" class=\"data row7 col2\" >2.05%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end = datetime.datetime.now()\n",
        "run_time = (end-start)\n",
        "print(run_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u6eAMb-UVXZ",
        "outputId": "19588dde-d295-4470-ccb9-984eebe99a9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:02:12.123449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    for name, module in model.named_modules():\n",
        "        print(name, \"->\", module.__class__.__name__)"
      ],
      "metadata": {
        "id": "2COJ34fKsJ1S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    print(model)"
      ],
      "metadata": {
        "id": "bQHDALvGsQbL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGK12Yv90p_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(\n",
        "    model,\n",
        "    input_size=(2, 16),    # (batch_size=2, seq_len=16)\n",
        "    dtypes=[torch.long],   # important! embeddings expect LongTensor\n",
        "    device=\"cpu\",          # or \"cuda\" if you’re on GPU\n",
        "    mode=\"eval\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uRecg_o0qEb",
        "outputId": "a95d037d-b35f-4c5a-e2fb-42d56ba506aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "SimpleTransformerLM                           [2, 16, 3]                --\n",
              "├─Embedding: 1-1                              [2, 16, 256]              768\n",
              "├─Embedding: 1-2                              [16, 256]                 (524,288)\n",
              "├─Dropout: 1-3                                [2, 16, 256]              --\n",
              "├─ModuleList: 1-4                             --                        --\n",
              "│    └─TransformerBlock: 2-1                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-1                    [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-2       [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-3                      [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-4                    [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-5                  [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-6                      [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-2                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-7                    [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-8       [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-9                      [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-10                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-11                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-12                     [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-3                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-13                   [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-14      [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-15                     [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-16                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-17                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-18                     [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-4                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-19                   [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-20      [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-21                     [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-22                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-23                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-24                     [2, 16, 256]              --\n",
              "├─LayerNorm: 1-5                              [2, 16, 256]              512\n",
              "├─Linear: 1-6                                 [2, 16, 3]                768\n",
              "===============================================================================================\n",
              "Total params: 3,685,376\n",
              "Trainable params: 3,161,088\n",
              "Non-trainable params: 524,288\n",
              "Total mult-adds (Units.MEGABYTES): 14.71\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 3.05\n",
              "Params size (MB): 14.74\n",
              "Estimated Total Size (MB): 17.79\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s3oatvz-0qKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hG1WkX_90qPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2\n",
        "B, T = 2, 16   # batch size 2, sequence length 16\n",
        "dummy_input = torch.randint(1, vocab_size, (B, T), dtype=torch.long).to(device)\n",
        "\n",
        "# print summary\n",
        "summary(model, input_data=dummy_input, mode=\"eval\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GceLqBPaxYHy",
        "outputId": "b2cf6134-1bdd-4eda-8746-8f05a2807d7d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "SimpleTransformerLM                           [2, 16, 3]                --\n",
              "├─Embedding: 1-1                              [2, 16, 256]              768\n",
              "├─Embedding: 1-2                              [16, 256]                 (524,288)\n",
              "├─Dropout: 1-3                                [2, 16, 256]              --\n",
              "├─ModuleList: 1-4                             --                        --\n",
              "│    └─TransformerBlock: 2-1                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-1                    [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-2       [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-3                      [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-4                    [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-5                  [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-6                      [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-2                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-7                    [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-8       [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-9                      [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-10                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-11                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-12                     [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-3                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-13                   [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-14      [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-15                     [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-16                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-17                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-18                     [2, 16, 256]              --\n",
              "│    └─TransformerBlock: 2-4                  [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-19                   [2, 16, 256]              512\n",
              "│    │    └─MultiHeadSelfAttention: 3-20      [2, 16, 256]              263,168\n",
              "│    │    └─Dropout: 3-21                     [2, 16, 256]              --\n",
              "│    │    └─LayerNorm: 3-22                   [2, 16, 256]              512\n",
              "│    │    └─FeedForward: 3-23                 [2, 16, 256]              525,568\n",
              "│    │    └─Dropout: 3-24                     [2, 16, 256]              --\n",
              "├─LayerNorm: 1-5                              [2, 16, 256]              512\n",
              "├─Linear: 1-6                                 [2, 16, 3]                768\n",
              "===============================================================================================\n",
              "Total params: 3,685,376\n",
              "Trainable params: 3,161,088\n",
              "Non-trainable params: 524,288\n",
              "Total mult-adds (Units.MEGABYTES): 14.71\n",
              "===============================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 3.05\n",
              "Params size (MB): 14.74\n",
              "Estimated Total Size (MB): 17.79\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zqBHIK1xYMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model)"
      ],
      "metadata": {
        "id": "0-rs_Suzve9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,input_size=(240,48))"
      ],
      "metadata": {
        "id": "OmiINwCttOSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = torch.tensor([12, 24], dtype=torch.long)\n",
        "summary(model,input_size=input_size)"
      ],
      "metadata": {
        "id": "HE5PwUfytSOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,input_size=(int(240),int(48)))"
      ],
      "metadata": {
        "id": "8tyFGN7lwogv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "d_model = 512\n",
        "model2 = nn.Transformer(\n",
        "    d_model=d_model, nhead=8,\n",
        "    num_encoder_layers=6, num_decoder_layers=6,\n",
        "    dim_feedforward=2048\n",
        ")\n",
        "\n",
        "S, T, N = 10, 9, 32\n",
        "src = torch.randn(S, N, d_model)  # floats are correct here\n",
        "tgt = torch.randn(T, N, d_model)\n",
        "\n",
        "summary(model2, input_data=(src, tgt), mode=\"eval\")\n"
      ],
      "metadata": {
        "id": "iPYmN3-9w0vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwWoJUVwxJGi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}