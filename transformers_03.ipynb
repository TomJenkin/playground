{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjDdX0O/dKm09BqIpQwbPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomJenkin/playground/blob/main/transformers_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bXWvG-5XQTwZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------\n",
        "# Positional Encodings (sin/cos as in the paper)\n",
        "# -----------------------\n",
        "def sinusoidal_position_encoding(max_len: int, d_model: int, device=None):\n",
        "    pe = torch.zeros(max_len, d_model, device=device)\n",
        "    position = torch.arange(0, max_len, device=device).unsqueeze(1)  # [T, 1]\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2, device=device) *\n",
        "                         (-math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe  # [T, d_model]\n",
        "\n",
        "# -----------------------\n",
        "# Multi-Head Self-Attention (causal)\n",
        "# -----------------------\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x: [B, T, d_model]\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)  # [B, T, 3C]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # reshape -> [B, n_heads, T, d_head]\n",
        "        def split_heads(t):\n",
        "            return t.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        q = split_heads(q)\n",
        "        k = split_heads(k)\n",
        "        v = split_heads(v)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # [B, h, T, T]\n",
        "        if attn_mask is not None:\n",
        "            scores = scores.masked_fill(attn_mask, float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        out = attn @ v  # [B, h, T, d_head]\n",
        "\n",
        "        # concat heads\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # [B, T, C]\n",
        "        out = self.proj_drop(self.out(out))\n",
        "        return out\n",
        "\n",
        "# -----------------------\n",
        "# Position-wise FeedForward\n",
        "# -----------------------\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.drop(F.relu(self.fc1(x))))  # ReLU as in the paper\n",
        "\n",
        "# -----------------------\n",
        "# Transformer Block (Pre-LN for stability)\n",
        "# -----------------------\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.sa = MultiHeadSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        x = x + self.drop(self.sa(self.ln1(x), attn_mask=attn_mask))\n",
        "        x = x + self.drop(self.ff(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "# -----------------------\n",
        "# Simplified Decoder-Only Transformer LM\n",
        "# -----------------------\n",
        "class SimpleTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=256, n_layers=4, n_heads=4, d_ff=1024,\n",
        "                 max_len=512, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_len = max_len\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(\n",
        "            sinusoidal_position_encoding(max_len, d_model),\n",
        "            freeze=True\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # weight tying\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "\n",
        "    def _make_attention_mask(self, x):\n",
        "        \"\"\"\n",
        "        Build a combined mask:\n",
        "        - causal mask to block attending to future positions\n",
        "        - key padding mask to block attending to pads in K/V\n",
        "        returns: mask with True where attention is NOT allowed, shape [B, 1, T, T]\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        # causal: [T, T] True above diagonal (i<j)\n",
        "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
        "        causal = causal.unsqueeze(0).unsqueeze(0)  # [1,1,T,T]\n",
        "\n",
        "        # key padding: mask positions that are pad in keys\n",
        "        key_pad = (x == self.pad_id).unsqueeze(1).unsqueeze(2)  # [B,1,1,T]\n",
        "\n",
        "        # broadcast OR\n",
        "        attn_mask = causal | key_pad  # [B,1,T,T]\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: LongTensor [B, T] with token ids (may include pad_id).\n",
        "        Returns logits for all positions: [B, T, vocab_size]\n",
        "        \"\"\"\n",
        "        B, T = x.shape\n",
        "        if T > self.max_len:\n",
        "            raise ValueError(f\"Sequence length {T} exceeds max_len {self.max_len}\")\n",
        "\n",
        "        device = x.device\n",
        "        pos = torch.arange(T, device=device)\n",
        "        h = self.tok_emb(x) * math.sqrt(self.d_model) + self.pos_emb(pos)  # [B,T,C]\n",
        "        h = self.drop(h)\n",
        "\n",
        "        attn_mask = self._make_attention_mask(x)  # [B,1,T,T]\n",
        "        for blk in self.blocks:\n",
        "            h = blk(h, attn_mask=attn_mask)\n",
        "\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)  # [B,T,V]\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_next_token(self, x, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Returns next-token logits/probs for each sequence.\n",
        "        x: LongTensor [B, T]\n",
        "        \"\"\"\n",
        "        logits = self.forward(x)[:, -1, :]  # last position per sequence\n",
        "        if temperature != 1.0:\n",
        "            logits = logits / temperature\n",
        "        if top_k is not None:\n",
        "            # top-k filtering for sampling; keep it generic\n",
        "            topk_vals, topk_idx = torch.topk(logits, k=top_k, dim=-1)\n",
        "            mask = torch.full_like(logits, float('-inf'))\n",
        "            logits = mask.scatter(1, topk_idx, topk_vals)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)  # [B, V]\n",
        "        next_token = torch.argmax(probs, dim=-1)  # greedy\n",
        "        return next_token, probs  # ints [B], floats [B,V]\n",
        "\n",
        "# -----------------------\n",
        "# Example usage\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # batch of sequences (variable lengths, padded with pad_id)\n",
        "    batch = [\n",
        "        [101, 42, 77, 88, 5],\n",
        "        [101, 19, 23],\n",
        "        [101, 202, 13, 9, 17, 4, 55]\n",
        "    ]\n",
        "\n",
        "    batch = [\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "        [1,1,2]*80,\n",
        "    ]\n",
        "\n",
        "    vocab = sorted(set.union(*[set(e) for e in batch]))\n",
        "\n",
        "    # toy config\n",
        "    assert min(vocab) > 0\n",
        "    vocab_size = max(vocab) + 1\n",
        "    # vocab_size = 32000\n",
        "    pad_id = 0\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = SimpleTransformerLM(\n",
        "        vocab_size=vocab_size,\n",
        "        #d_model=256,\n",
        "        d_model=256,\n",
        "        #n_layers=4,\n",
        "        n_layers=4,\n",
        "        #n_heads=4,\n",
        "        n_heads=4,\n",
        "        #d_ff=1024,\n",
        "        d_ff=1024,\n",
        "        max_len=256,\n",
        "        dropout=0.1,\n",
        "        pad_id=pad_id\n",
        "    ).to(device)\n",
        "\n",
        "    # pad right to same length\n",
        "    maxT = max(len(s) for s in batch)\n",
        "    x = torch.full((len(batch), maxT), pad_id, dtype=torch.long)\n",
        "    for i, s in enumerate(batch):\n",
        "        x[i, :len(s)] = torch.tensor(s, dtype=torch.long)\n",
        "    x = x.to(device)\n",
        "\n",
        "    # ---- quick warm-up training so the model learns 1,1,2 -> 1 ----\n",
        "    model.train()\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    # teacher-forcing targets: predict next token\n",
        "    # inp means inputs, targ means target\n",
        "    inp  = x[:, :-1]          # [B, T-1]\n",
        "    targ = x[:, 1:]           # [B, T-1]\n",
        "\n",
        "    steps = 300\n",
        "    for _ in range(steps):\n",
        "        logits = model(inp)               # [B, T-1, V]\n",
        "        loss = loss_fn(logits.reshape(-1, model.vocab_size), targ.reshape(-1))\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    model.eval()\n",
        "    # ---- end warm-up training ----\n",
        "\n",
        "    # predict the next token for each sequence\n",
        "    next_ids, next_probs = model.predict_next_token(x, temperature=1.0)\n",
        "    print(\"Next token ids:\", next_ids.tolist())\n",
        "    # next_probs[i] is the full distribution for sequence i\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJlCyKlcQe5H",
        "outputId": "a33cc812-5f87-4785-cedd-00123f6d2ec3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token ids: [1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LWMk3rsvi5EU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_ids, next_probs = model.predict_next_token(x, temperature=1.0)\n",
        "print(\"Next token ids:\", next_ids.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx-xDWoqnSNP",
        "outputId": "3162aba8-2a00-4ed6-d5c2-6d96525781e4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next token ids: [1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(next_probs.cpu()).style.format('{:,.2%}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "tEosrgLonoMu",
        "outputId": "bc825ece-bfd3-4f51-c5e8-494965bf4b3d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f72f0977c80>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_54b49\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_54b49_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
              "      <th id=\"T_54b49_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
              "      <th id=\"T_54b49_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_54b49_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_54b49_row0_col0\" class=\"data row0 col0\" >0.00%</td>\n",
              "      <td id=\"T_54b49_row0_col1\" class=\"data row0 col1\" >99.98%</td>\n",
              "      <td id=\"T_54b49_row0_col2\" class=\"data row0 col2\" >0.02%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_54b49_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_54b49_row1_col0\" class=\"data row1 col0\" >0.00%</td>\n",
              "      <td id=\"T_54b49_row1_col1\" class=\"data row1 col1\" >99.98%</td>\n",
              "      <td id=\"T_54b49_row1_col2\" class=\"data row1 col2\" >0.02%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_54b49_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_54b49_row2_col0\" class=\"data row2 col0\" >0.00%</td>\n",
              "      <td id=\"T_54b49_row2_col1\" class=\"data row2 col1\" >99.98%</td>\n",
              "      <td id=\"T_54b49_row2_col2\" class=\"data row2 col2\" >0.02%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_54b49_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_54b49_row3_col0\" class=\"data row3 col0\" >0.00%</td>\n",
              "      <td id=\"T_54b49_row3_col1\" class=\"data row3 col1\" >99.98%</td>\n",
              "      <td id=\"T_54b49_row3_col2\" class=\"data row3 col2\" >0.02%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAjU_8dvTnji"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8u6eAMb-UVXZ"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}